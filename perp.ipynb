{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import whisper\n",
    "import torch\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "load_dotenv()\n",
    "google_api_key= os.getenv(\"GOOGLE_API_KEY\")\n",
    "hugging_face_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CallInteraction:\n",
    "    def __init__(self,api_key,base_url,model_name):\n",
    "        self.audio_model = whisper.load_model(\"base\")\n",
    "        self.Summary_model = OpenAI(api_key=api_key,base_url=base_url)\n",
    "        self.system_prompt = \"\"\"You are a summarization model. Your task is to analyze the following conversation or call transcript and generate a concise summary that captures the most important takeaways, key decisions, action items, and any notable concerns or questions raised during the discussion.\n",
    "\n",
    "            Guidelines:\n",
    "            Focus only on relevant and impactful information.\n",
    "            Do not include small talk or greetings.\n",
    "            Clearly identify who made key points or decisions (if possible).\n",
    "            Present the summary in bullet points or short paragraphs for clarity.\n",
    "            Maintain a professional and objective tone.\n",
    "\n",
    "            Expected Output:\n",
    "            Summary of the most important points\n",
    "            Action items (if any)\n",
    "            Key decisions made\n",
    "            Any questions or concerns raised\"\"\"\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def transcribe(self,audio_path):\n",
    "        result = self.audio_model.transcribe(audio_path)\n",
    "        return result[\"text\"]\n",
    "\n",
    "    def summarize_call(self,audio):\n",
    "        transcript = self.transcribe(audio)\n",
    "        message = [{\"role\":\"system\",\"content\":self.system_prompt}]+[{\"role\":\"user\",\"content\":transcript}]\n",
    "        response = self.Summary_model.chat.completions.create(messages=message,model=self.model_name)\n",
    "        summary = response.choices[0].message.content\n",
    "\n",
    "        return transcript,summary\n"
   ],
   "id": "9408be12f3943a0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "call_summarization = CallInteraction(google_api_key,base_url,model_name)\n",
    "#transcript,summary =call_summarization.summarize_call(\"Joe Rogan Experience 1598 - The Undertaker.mp3\")"
   ],
   "id": "f007f18107cd1469",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üéôÔ∏è Call Summarizer: Record or Upload Audio\")\n",
    "\n",
    "\n",
    "    audio_input = gr.Audio(\n",
    "        sources=[\"microphone\", \"upload\"],\n",
    "        type=\"filepath\",\n",
    "        label=\"Record or Upload Call\"\n",
    "    )\n",
    "\n",
    "    transcript_output = gr.Textbox(label=\"Transcription\", lines=8)\n",
    "    summary_output = gr.Textbox(label=\"Summary\", lines=8)\n",
    "\n",
    "    summarize_button = gr.Button(\"Summarize Call\")\n",
    "\n",
    "    summarize_button.click(\n",
    "        fn=call_summarization.summarize_call,\n",
    "        inputs=audio_input,\n",
    "        outputs=[transcript_output, summary_output]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ],
   "id": "7ec769716e53221c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#",
   "id": "59d23b7478ca9401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Seperation",
   "id": "fdc7321fa725510f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from moviepy import *\n",
    "\n",
    "def convert_mp4_to_mp3(mp4_path, mp3_path):\n",
    "    try:\n",
    "        video = VideoFileClip(\"interview.mkv\")\n",
    "        audio = video.audio\n",
    "        audio.write_audiofile(mp3_path)\n",
    "        print(f\"Conversion complete: {mp3_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "convert_mp4_to_mp3(\"input_video.mp4\", \"output_audio.mp3\")\n"
   ],
   "id": "6310230fc70230aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "9d27c503daa95694",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Seperation time",
   "id": "798c67526f1040fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from functools import partial\n",
    "\n",
    "# Patch torch.load to use weights_only=False\n",
    "original_load = torch.load\n",
    "torch.load = partial(original_load, weights_only=False)\n",
    "\n",
    "try:\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=hugging_face_token\n",
    "    ).to(torch.device(\"cuda\"))\n",
    "finally:\n",
    "    # Restore original torch.load\n",
    "    torch.load = original_load"
   ],
   "id": "e25df669e4f7644a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "diarization = pipeline(\"English Speaking.mp3\")",
   "id": "51bf94c59d919e78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")"
   ],
   "id": "96e6c512f61b5082",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import whisper\n",
    "\n",
    "# Load Whisper\n",
    "model = whisper.load_model(\"base\").to(torch.device(\"cuda\"))\n",
    "# Transcribe\n"
   ],
   "id": "54a9e1bfbade10f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "result = model.transcribe(\"BBC.mp3\", language=\"en\", verbose=True)\n",
    "segments = result['segments']"
   ],
   "id": "e01ed1abf16e373f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# Testing",
   "id": "87ef576b67a7354c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing",
   "id": "5eb80d92d3aa335"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "from functools import partial\n",
    "import whisper\n",
    "from datetime import timedelta\n",
    "\n",
    "# -- Settings --\n",
    "AUDIO_FILE = \"AudioFiles/English Speaking.mp3\"\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "# Patch for pyannote\n",
    "original_load = torch.load\n",
    "torch.load = partial(original_load, weights_only=False)\n",
    "\n",
    "# Load diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=hugging_face_token\n",
    ").to(DEVICE)\n",
    "\n",
    "# Restore original torch.load\n",
    "torch.load = original_load\n",
    "\n",
    "# Run diarization\n",
    "diarization = pipeline(AUDIO_FILE)\n",
    "\n",
    "# Convert diarization into list of dicts\n",
    "speaker_segments = []\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    speaker_segments.append({\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end,\n",
    "        \"speaker\": speaker\n",
    "    })\n",
    "\n",
    "# Load Whisper model\n",
    "model = whisper.load_model(\"base\").to(DEVICE)\n",
    "\n",
    "# Transcribe\n",
    "result = model.transcribe(AUDIO_FILE, language=\"en\", verbose=False)\n",
    "segments = result[\"segments\"]\n",
    "\n",
    "# Match each Whisper segment to a speaker\n",
    "final_output = []\n",
    "\n",
    "def find_speaker(start_time, end_time, speaker_segments):\n",
    "    for segment in speaker_segments:\n",
    "        # If the midpoint of the whisper segment lies inside a diarization segment\n",
    "        mid_point = (start_time + end_time) / 2\n",
    "        if segment[\"start\"] <= mid_point <= segment[\"end\"]:\n",
    "            return segment[\"speaker\"]\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "for seg in segments:\n",
    "    speaker = find_speaker(seg['start'], seg['end'], speaker_segments)\n",
    "    text = seg['text'].strip()\n",
    "    start_time = str(timedelta(seconds=int(seg['start'])))\n",
    "    end_time = str(timedelta(seconds=int(seg['end'])))\n",
    "    final_output.append({\n",
    "        \"speaker\": speaker,\n",
    "        \"start\": start_time,\n",
    "        \"end\": end_time,\n",
    "        \"text\": text\n",
    "    })\n",
    "\n",
    "# Print output\n",
    "# -- Format the transcript in a way that groups text by each speaker --\n",
    "speaker_map = {}\n",
    "speaker_counter = 1\n",
    "\n",
    "# Prepare for grouped speaker text\n",
    "grouped_transcript = []\n",
    "\n",
    "current_speaker = None\n",
    "accumulated_text = []\n",
    "\n",
    "for entry in final_output:\n",
    "    speaker_raw = entry[\"speaker\"]\n",
    "\n",
    "    # Map speakers\n",
    "    if speaker_raw not in speaker_map:\n",
    "        speaker_map[speaker_raw] = f\"Speaker {speaker_counter}\"\n",
    "        speaker_counter += 1\n",
    "    speaker_label = speaker_map[speaker_raw]\n",
    "\n",
    "    # If speaker changes, save previous accumulated text\n",
    "    if speaker_label != current_speaker:\n",
    "        if current_speaker is not None:  # Only save if it's not the first entry\n",
    "            grouped_transcript.append(f\"{current_speaker}: {' '.join(accumulated_text)}\")\n",
    "        current_speaker = speaker_label\n",
    "        accumulated_text = [entry['text']]  # Start accumulating new speaker's text\n",
    "    else:\n",
    "        accumulated_text.append(entry['text'])\n",
    "\n",
    "# Don't forget to append the last speaker's text\n",
    "if accumulated_text:\n",
    "    grouped_transcript.append(f\"{current_speaker}: {' '.join(accumulated_text)}\")\n",
    "\n",
    "# Join everything into the final output format\n",
    "summarization_input = \"\\n\\n\".join(grouped_transcript)\n",
    "\n",
    "# Print the result\n",
    "print(summarization_input)\n",
    "\n",
    "# Optional: save to a text file\n",
    "# with open(\"formatted_transcript_for_llm.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(summarization_input)\n"
   ],
   "id": "5a0d4a4366efa8ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(final_output)",
   "id": "b2203bb867c6c89d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "aa7ef0f18b7aa76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# OOP",
   "id": "4b87b697ed51512f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:24:31.647139Z",
     "start_time": "2025-05-15T07:24:31.630193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "from pyannote.audio import Pipeline\n",
    "from functools import partial\n",
    "import whisper\n",
    "from datetime import timedelta\n",
    "\n",
    "class SpeakerDiarization:\n",
    "    def __init__(self, audio_file, device, hugging_face_token):\n",
    "        self.audio_file = audio_file\n",
    "        self.device = device\n",
    "        self.hugging_face_token = hugging_face_token\n",
    "        self.pipeline = self._load_diarization_pipeline()\n",
    "\n",
    "    def _load_diarization_pipeline(self):\n",
    "        # Patch for pyannote\n",
    "        original_load = torch.load\n",
    "        torch.load = partial(original_load, weights_only=False)\n",
    "\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=self.hugging_face_token\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Restore original torch.load\n",
    "        torch.load = original_load\n",
    "\n",
    "        return pipeline\n",
    "\n",
    "    def diarize(self):\n",
    "        diarization = self.pipeline(self.audio_file)\n",
    "        speaker_segments = []\n",
    "        for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "            speaker_segments.append({\n",
    "                \"start\": turn.start,\n",
    "                \"end\": turn.end,\n",
    "                \"speaker\": speaker\n",
    "            })\n",
    "        return speaker_segments\n",
    "\n",
    "class SpeechToText:\n",
    "    def __init__(self, audio_file, device):\n",
    "        self.audio_file = audio_file\n",
    "        self.device = device\n",
    "        self.model = whisper.load_model(\"base\").to(self.device)\n",
    "\n",
    "    def transcribe(self):\n",
    "        result = self.model.transcribe(self.audio_file, language=\"en\", verbose=False)\n",
    "        return result[\"segments\"]\n",
    "\n",
    "class SpeakerTextMapper:\n",
    "    def __init__(self, speaker_segments, transcribed_segments):\n",
    "        self.speaker_segments = speaker_segments\n",
    "        self.transcribed_segments = transcribed_segments\n",
    "\n",
    "    def find_speaker(self, start_time, end_time):\n",
    "        for segment in self.speaker_segments:\n",
    "            # If the midpoint of the whisper segment lies inside a diarization segment\n",
    "            mid_point = (start_time + end_time) / 2\n",
    "            if segment[\"start\"] <= mid_point <= segment[\"end\"]:\n",
    "                return segment[\"speaker\"]\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "    def map_speakers(self):\n",
    "        final_output = []\n",
    "        for seg in self.transcribed_segments:\n",
    "            speaker = self.find_speaker(seg['start'], seg['end'])\n",
    "            text = seg['text'].strip()\n",
    "            start_time = str(timedelta(seconds=int(seg['start'])))\n",
    "            end_time = str(timedelta(seconds=int(seg['end'])))\n",
    "            final_output.append({\n",
    "                \"speaker\": speaker,\n",
    "                \"start\": start_time,\n",
    "                \"end\": end_time,\n",
    "                \"text\": text\n",
    "            })\n",
    "        return final_output\n",
    "\n",
    "class TranscriptFormatter:\n",
    "    def __init__(self, final_output):\n",
    "        self.final_output = final_output\n",
    "\n",
    "    def format_transcript(self):\n",
    "        # Map for speaker names\n",
    "        speaker_map = {}\n",
    "        speaker_counter = 1\n",
    "\n",
    "        # Prepare for grouped speaker text\n",
    "        grouped_transcript = []\n",
    "\n",
    "        current_speaker = None\n",
    "        accumulated_text = []\n",
    "\n",
    "        for entry in self.final_output:\n",
    "            speaker_raw = entry[\"speaker\"]\n",
    "\n",
    "            # Map speakers\n",
    "            if speaker_raw not in speaker_map:\n",
    "                speaker_map[speaker_raw] = f\"Speaker {speaker_counter}\"\n",
    "                speaker_counter += 1\n",
    "            speaker_label = speaker_map[speaker_raw]\n",
    "\n",
    "            # If speaker changes, save previous accumulated text\n",
    "            if speaker_label != current_speaker:\n",
    "                if current_speaker is not None:  # Only save if it's not the first entry\n",
    "                    grouped_transcript.append(f\"{current_speaker}: {' '.join(accumulated_text)}\")\n",
    "                current_speaker = speaker_label\n",
    "                accumulated_text = [entry['text']]  # Start accumulating new speaker's text\n",
    "            else:\n",
    "                accumulated_text.append(entry['text'])\n",
    "\n",
    "        # Don't forget to append the last speaker's text\n",
    "        if accumulated_text:\n",
    "            grouped_transcript.append(f\"{current_speaker}: {' '.join(accumulated_text)}\")\n",
    "\n",
    "        # Join everything into the final output format\n",
    "        return \"\\n\\n\".join(grouped_transcript)\n",
    "\n",
    "# class AudioProcessor:\n",
    "#     def __init__(self, audio_file, device, hugging_face_token):\n",
    "#         self.audio_file = audio_file\n",
    "#         self.device = device\n",
    "#         self.hugging_face_token = hugging_face_token\n",
    "#\n",
    "#     def process_audio(self):\n",
    "#         # Diarization\n",
    "#         diarization = SpeakerDiarization(self.audio_file, self.device, self.hugging_face_token)\n",
    "#         speaker_segments = diarization.diarize()\n",
    "#\n",
    "#         # Transcription\n",
    "#         speech_to_text = SpeechToText(self.audio_file, self.device)\n",
    "#         transcribed_segments = speech_to_text.transcribe()\n",
    "#\n",
    "#         # Map speakers to transcribed segments\n",
    "#         speaker_text_mapper = SpeakerTextMapper(speaker_segments, transcribed_segments)\n",
    "#         final_output = speaker_text_mapper.map_speakers()\n",
    "#\n",
    "#         # Format the transcript\n",
    "#         transcript_formatter = TranscriptFormatter(final_output)\n",
    "#         Full_transcript = transcript_formatter.format_transcript()\n",
    "#\n",
    "#         return Full_transcript\n",
    "\n",
    "class SummaryEvaluator:\n",
    "    def __init__(self, api_key: str, base_url: str, model_name: str):\n",
    "        self.client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "        self.model_name = model_name\n",
    "        self.system_prompt = self._build_system_prompt()\n",
    "\n",
    "    def _build_system_prompt(self) -> str:\n",
    "        return (\n",
    "            \"You are an expert assistant for evaluating the quality of generated summaries from audio or meeting transcripts. \"\n",
    "            \"Below is the original transcript of a conversation, followed by a summary generated by a model. Your task is to \"\n",
    "            \"objectively evaluate the summary based on the following criteria:\\n\\n\"\n",
    "            \"Evaluation Criteria:\\n\\n\"\n",
    "            \"Coverage: Does the summary capture the most important points, decisions, and action items from the transcript?\\n\\n\"\n",
    "            \"Faithfulness: Is the summary factually accurate and consistent with the content of the transcript (i.e., no hallucinations or distortions)?\\n\\n\"\n",
    "            \"Clarity: Is the summary easy to read, well-organized, and written in a professional tone?\\n\\n\"\n",
    "            \"Conciseness: Is the summary concise without omitting critical information?\\n\\n\"\n",
    "            \"Speaker Attribution (optional): Where relevant, does the summary correctly attribute key statements or decisions to the right speaker?\\n\\n\"\n",
    "            \"With these criteria, please evaluate the summary, replying with whether the summary is acceptable and your feedback.\"\n",
    "        )\n",
    "\n",
    "    def _build_user_prompt(self, transcript: str, summary: str) -> str:\n",
    "        return (\n",
    "            f\"Here is the full transcript of the call:\\n\\n{transcript}\\n\\n\"\n",
    "            f\"Here is the summary of the call:\\n\\n{summary}\\n\\n\"\n",
    "            \"Please evaluate the summary, replying with whether it is acceptable and your feedback.\"\n",
    "        )\n",
    "\n",
    "    def evaluate(self, transcript: str, summary: str) -> Evaluation:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": self._build_user_prompt(transcript, summary)}\n",
    "        ]\n",
    "\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            messages=messages,\n",
    "            model=self.model_name,\n",
    "            response_format=Evaluation\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.parsed\n",
    "\n",
    "class CallInteraction:\n",
    "    def __init__(\n",
    "        self,\n",
    "        audio_file,\n",
    "        device,\n",
    "        hugging_face_token,\n",
    "        api_key,\n",
    "        base_url,\n",
    "        model_name,\n",
    "        prompt_template=None,\n",
    "        evaluation_api_key=None,\n",
    "        evaluation_base_url=None,\n",
    "        evaluation_model_name=None\n",
    "    ):\n",
    "        self.audio_file = audio_file\n",
    "        self.device = device\n",
    "        self.hugging_face_token = hugging_face_token\n",
    "        self.Summary_model = OpenAI(api_key=api_key,base_url=base_url)\n",
    "        self.system_prompt = prompt_template or self.default_prompt()\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.evaluator = SummaryEvaluator(\n",
    "            api_key=evaluation_api_key or api_key,\n",
    "            base_url=evaluation_base_url or base_url,\n",
    "            model_name=evaluation_model_name or model_name\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    def default_prompt(self):\n",
    "        return (\n",
    "            \"You are a summarization model. Your task is to analyze the following conversation \"\n",
    "            \"or call transcript and generate a concise summary that captures the most important takeaways, \"\n",
    "            \"key decisions, action items, and any notable concerns or questions raised during the discussion.\\n\\n\"\n",
    "            \"Guidelines:\\n\"\n",
    "            \"- Focus only on relevant and impactful information.\\n\"\n",
    "            \"- Do not include small talk or greetings.\\n\"\n",
    "            \"- Clearly identify who made key points or decisions (if possible).\\n\"\n",
    "            \"- Present the summary in bullet points or short paragraphs for clarity.\\n\"\n",
    "            \"- Maintain a professional and objective tone.\\n\\n\"\n",
    "            \"Expected Output:\\n\"\n",
    "            \"- Summary of the most important points\\n\"\n",
    "            \"- Action items (if any)\\n\"\n",
    "            \"- Key decisions made\\n\"\n",
    "            \"- Any questions or concerns raised\\n\\n\"\n",
    "            \"Transcript:\\n\\n\"\n",
    "        )\n",
    "\n",
    "    def updated_system_prompt(self,transcript,summary,feedback):\n",
    "        updated_prompt = self.system_prompt + f\"\\n\\n## Previous summary was rejected\\nYou just tried to reply, but the quality control rejected your summary\\n\"\n",
    "        updated_prompt += f\"## Your attempted summary:\\n{summary}\\n\\n\"\n",
    "        updated_prompt+=f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "        return updated_prompt\n",
    "\n",
    "    def summarize_call(self):\n",
    "        diarization = SpeakerDiarization(self.audio_file, self.device, self.hugging_face_token)\n",
    "        speaker_segments = diarization.diarize()\n",
    "\n",
    "        # Transcription\n",
    "        speech_to_text = SpeechToText(self.audio_file, self.device)\n",
    "        transcribed_segments = speech_to_text.transcribe()\n",
    "\n",
    "        # Map speakers to transcribed segments\n",
    "        speaker_text_mapper = SpeakerTextMapper(speaker_segments, transcribed_segments)\n",
    "        final_output = speaker_text_mapper.map_speakers()\n",
    "\n",
    "        # Format the transcript\n",
    "        transcript_formatter = TranscriptFormatter(final_output)\n",
    "        full_transcript = transcript_formatter.format_transcript()\n",
    "\n",
    "        message = [{\"role\":\"system\",\"content\":self.system_prompt}]+[{\"role\":\"user\",\"content\":full_transcript}]\n",
    "        response = self.Summary_model.chat.completions.create(messages=message,model=self.model_name)\n",
    "        summary = response.choices[0].message.content\n",
    "\n",
    "        evaluation = self.evaluator.evaluate(full_transcript, summary)\n",
    "\n",
    "        if not evaluation.is_acceptable:\n",
    "            # Retry with updated prompt\n",
    "            updated_prompt = self.updated_system_prompt(full_transcript, summary, evaluation.feedback)\n",
    "            retry_messages = [\n",
    "                {\"role\": \"system\", \"content\": updated_prompt},\n",
    "                {\"role\": \"user\", \"content\": full_transcript}\n",
    "            ]\n",
    "            retry_response = self.Summary_model.chat.completions.create(messages=retry_messages, model=self.model_name)\n",
    "            summary = retry_response.choices[0].message.content\n",
    "            evaluation = self.evaluator.evaluate(full_transcript, summary)\n",
    "\n",
    "        return full_transcript, summary, evaluation.feedback\n",
    "\n"
   ],
   "id": "73f18256bddb162c",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T07:25:06.597978Z",
     "start_time": "2025-05-15T07:25:06.414789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def process_uploaded_audio(audio_file):\n",
    "    interaction = CallInteraction(\n",
    "        audio_file=audio_file,\n",
    "        device=device,\n",
    "        hugging_face_token=hugging_face_token,\n",
    "        api_key=google_api_key,\n",
    "        base_url=base_url,\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return interaction.summarize_call()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## üéôÔ∏è Call Summarizer: Record or Upload Audio\")\n",
    "\n",
    "    audio_input = gr.Audio(\n",
    "        sources=[\"microphone\", \"upload\"],\n",
    "        type=\"filepath\",\n",
    "        label=\"Record or Upload Call\"\n",
    "    )\n",
    "\n",
    "    transcript_output = gr.Textbox(label=\"Transcription\", lines=8)\n",
    "    summary_output = gr.Textbox(label=\"Summary\", lines=8)\n",
    "    feedback_output = gr.Textbox(label=\"LLM Evaluation Feedback\", lines=5)\n",
    "\n",
    "    summarize_button = gr.Button(\"Summarize Call\")\n",
    "\n",
    "    summarize_button.click(\n",
    "        fn=process_uploaded_audio,\n",
    "        inputs=audio_input,\n",
    "        outputs=[transcript_output, summary_output, feedback_output]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ],
   "id": "33ca7c11e5ccb839",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8b4c878d7caff39"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
